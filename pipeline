import os
import torch
import json
import requests
from bs4 import BeautifulSoup
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
from openai import OpenAI
from google.colab import drive, userdata
from diffusers import StableDiffusionPipeline
from IPython.display import Image, display

print("Step 1: Installing and upgrading libraries...")


print("\nStep 2: Mounting Google Drive...")
drive.mount('/content/drive')

print("\nStep 3: Loading fine-tuned KoBERT model and tokenizer...")
try:
    MODEL_PATH = '/content/drive/MyDrive/project_DL/final_kobert_classifier_v2'
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    tokenizer_kobert = AutoTokenizer.from_pretrained(MODEL_PATH)
    model_kobert = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH).to(device)
    
    label_map_path = os.path.join(MODEL_PATH, "label_map.json")
    with open(label_map_path, 'r', encoding='utf-8') as f:
        label_map = json.load(f)
    print("KoBERT model loaded successfully.")
except Exception as e:
    print(f"Error loading KoBERT model: {e}")
    tokenizer_kobert, model_kobert, label_map = None, None, {}

print("\nStep 4: Setting up OpenAI client...")
try:
    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')
    client = OpenAI(api_key=OPENAI_API_KEY)
    print("OpenAI client setup successful.")
except Exception as e:
    print(f"Could not set up OpenAI client: {e}")
    client = None

print("\nStep 5: Loading Stable Diffusion model (This may take a long time on first run)...")
try:
    device = "cuda" if torch.cuda.is_available() else "cpu"
    sd_pipe = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16)
    sd_pipe = sd_pipe.to(device)
    print("Stable Diffusion model loaded successfully.")
except Exception as e:
    print(f"Error loading Stable Diffusion model: {e}")
    sd_pipe = None

print("\nStep 6: Defining pipeline functions...")

def scrape_news_article(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        for tag in soup(["script", "style"]):
            tag.decompose()
            
        article_text_parts = []
        possible_tags = ['p', 'div', 'span']
        possible_classes = ['article_body', 'news_content', 'article_text', 'news_view', 'view_content', 'content_area', 'ar_txt']
        
        for tag in possible_tags:
            for cls in possible_classes:
                found_elements = soup.find_all(tag, class_=cls)
                for element in found_elements:
                    text = element.get_text(separator='\n', strip=True)
                    if text:
                        article_text_parts.append(text)
        
        import re
        div_article_element = soup.find('div', id=re.compile(r'article_contents|dic_area'))
        if div_article_element:
            text = div_article_element.get_text(separator='\n', strip=True)
            if text:
                article_text_parts.append(text)

        unique_sentences = []
        for paragraph in article_text_parts:
            sentences = re.split(r'(?<=[.?!])\s+', paragraph)
            for sentence in sentences:
                sentence = sentence.strip()
                if sentence and len(sentence) > 10 and sentence not in unique_sentences:
                    unique_sentences.append(sentence)
        
        final_text = '\n'.join(unique_sentences)
        if len(final_text) > 4000:
            final_text = final_text[:4000] + "..."
            
        return final_text if final_text else None

    except requests.exceptions.RequestException as e:
        print(f"URL 접근 또는 스크래핑 오류: {e}")
        return None
    except Exception as e:
        print(f"기사 스크래핑 중 알 수 없는 오류 발생: {e}")
        return None

def classify_category(text):
    if not model_kobert or not tokenizer_kobert: return "분류 모델 로딩 실패"
    
    max_len = 512

    inputs = tokenizer_kobert(
        text,
        truncation=True,
        padding="max_length",
        max_length=max_len,
        return_tensors="pt",
        return_token_type_ids=False,
    )
    inputs = {k: v.to(device) for k, v in inputs.items()}
    with torch.no_grad():
        logits = model_kobert(**inputs).logits
    pred_label_idx = logits.argmax(dim=-1).item()
    
    return label_map.get(str(pred_label_idx), "알 수 없음")

def summarize_with_gpt3_5(text):
    if not client or not text: return "요약 불가"
    prompt = f"""당신은 대한민국의 주요 언론사의 전문 뉴스 편집자입니다. 주어진 뉴스 기사의 핵심 내용을 요약하는 역할을 수행합니다.

** 준수 사항 **
1. 기사의 내용을 뉴스 기사의 양에 따라 4~5줄로 명확하게 요약해야함
2. 실제 기자처럼 중립적이고 객관적인 어조를 유지해야함
3. 육하원칙(언제, 어디서, 누가, 무엇을, 어떻게, 왜)에 해당하는 핵심 정보 위주로 요약해야함
4. 인과 관계 및 상관관계에 해당하는 문장이 있고 요약해야하는 상황이 명확히 요약 적어야함
5. 요약문은 반드시 한국어로 작성해야함

** 원본 기사 **
{text}

** 요약 결과 (한국어) **
"""
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "당신은 대한민국의 주요 언론사의 전문 뉴스 편집자입니다."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7, max_tokens=1024
        )
        summary = response.choices[0].message.content.strip()
        
        lines = summary.split('\n')
        if len(lines) > 5:
            summary = '\n'.join(lines[:5])
        return summary
    except Exception as e:
        print(f"GPT-3.5 API 호출 오류: {e}")
        return "요약 중 오류가 발생했습니다."

def translate_to_english(text):
    if not client or not text: return "Translation failed"
    try:
        prompt = f"Translate the following Korean text to English:\n\nKorean text:\n{text}"
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=1024
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"Translation with OpenAI failed: {e}")
        return "Translation failed"

def generate_image(category, summary_en):
    if not sd_pipe: return None, "이미지 생성 실패: Stable Diffusion 모델 로드 오류"
    prompt = f"dramatic cinematic lighting, digital art, concept art, hyperrealistic, photorealistic, an image that visualizes the news topic of '{category}', based on the theme '{summary_en}'"
    try:
        image = sd_pipe(prompt).images[0]
        image_path = "generated_thumbnail.png"
        image.save(image_path)
        return image_path, "이미지 생성 완료"
    except Exception as e:
        print(f"Error generating image: {e}")
        return None, "이미지 생성 실패"

print("All functions defined.")

print("\nStep 7: Starting the full pipeline...")
if model_kobert and tokenizer_kobert and client and sd_pipe:
    news_url = input("분석할 뉴스 기사 URL을 입력하세요: ")
    article_text = scrape_news_article(news_url)

    if article_text:
        print("\n--- 뉴스 기사 분석 진행 중 ---")
        
        print("\n[ 원본 기사 (일부) ]")
        print(article_text[:500] + "...")
        
        category = classify_category(article_text)
        summary_ko = summarize_with_gpt3_5(article_text)
        
        print("\n[ GPT-3.5 요약 결과 ]")
        print(summary_ko)

        summary_en = translate_to_english(summary_ko)
        if summary_en == "Translation failed":
            print("\n[ 번역 실패 ] 이미지 생성을 위해 영어 요약본이 필요합니다.")
            image_path = None
            image_status = "이미지 생성 실패 (번역 오류)"
        else:
            image_path, image_status = generate_image(category, summary_en)
        
        print("\n\n--- 최종 결과물 ---")
        print(f"\n[ 카테고리 ]\n{category}")
        print("\n[ 뉴스 기사 요약]")
        print(summary_ko)
        print(f"\n[ 이미지 ]\n{image_status}")
        if image_path:
            display(Image(filename=image_path))
    else:
        print("스크래핑 실패. URL을 확인해주세요.")
else:
    print("파이프라인 실행 실패: 필요한 모델 또는 클라이언트가 준비되지 않았습니다.")
